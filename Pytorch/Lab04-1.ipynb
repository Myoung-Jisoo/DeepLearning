{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "revised-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-performance",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-landscape",
   "metadata": {},
   "source": [
    "여러 개의 정보로부터 하나의 추측값을 계산하는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-classic",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "### H(x) = Wx + b\n",
    "#### # of hours studied (1)  ->  ■  ->  Test score (2)\n",
    "'한 시간을 공부하면 2점이라는 시험 성적을 기대할 수 있을 것이다.' 라고 예측할 수 있는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-force",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-uncertainty",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression\n",
    "다양한 정보를 가지고 예측을 할 수 있음\n",
    "\n",
    "복수의 정보다 존재할 때어떻게 하나의 추측값을 계산할 수 있는가?\n",
    "\n",
    "### H(x) = Wx + b\n",
    "#### Quiz Scores (73, 80, 75)  ->  ■  -> Final Score Prediction (152)\n",
    "'쪽지시험 별 성적이 73, 80, 75인 학생이 기말고사 점수 몇점이 나올 것인가?'를 예측하는 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-withdrawal",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-contemporary",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "duplicate-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-stocks",
   "metadata": {},
   "source": [
    "다섯 명의 학생이 세 번의 쪽지시험에서 받은 점수들과 기말고사 점수들을 가지고, 쪽지시험 점수를 받았을 때 기말고사 점수를 예측하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-circumstances",
   "metadata": {},
   "source": [
    "## Hypothesis Function 인공 신경망의 구조\n",
    "\n",
    "### H(x) = Wx + b\n",
    "x라는 vector와 W라는 matrix의 곱\n",
    "### H(x) = ω₁x₁ + ω₂x₂ + ω₃x₃ + b\n",
    "입력 변수가 3개면 weight도 3개 => W와 x 모두 3 x 1 벡터로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sunrise-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])\n",
    "y1_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "x2_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])\n",
    "y2_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "x3_train = torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])\n",
    "y3_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "W1 = torch.zeros(1, requires_grad = True)\n",
    "W2 = torch.zeros(1, requires_grad = True)\n",
    "W3 = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True) # W와 b 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patient-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H(x) 계산\n",
    "hypothesis = x1_train * W1 + x2_train * W2 + x3_train * W3 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-buyer",
   "metadata": {},
   "source": [
    "## Hypothesis Function : Naive\n",
    "#### - 단순한 hypothesis 정의\n",
    "#### - 하지만 x가 길이 1000의 vector라면? => hypothesis 계산식이 너무 길어짐!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-communist",
   "metadata": {},
   "source": [
    "## Hypothesis Function : Matrix\n",
    "### - matmul()로 한번에 계산 (Matmul : Matrix multipication)\n",
    "####   더 간결\n",
    "####   x의 길이가 바뀌어도 코드를 바꿀 필요X\n",
    "####   속도가 더 빠르다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beginning-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True) # W와 b 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "preliminary-ukraine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, get 5, 5x3,1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-81f866cc15a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# H(x)로 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhypothesis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;31m# or .mm or @\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, get 5, 5x3,1"
     ]
    }
   ],
   "source": [
    "# H(x)로 계산\n",
    "hypothesis = x_train.matmul(W) + b # or .mm or @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-therapy",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-establishment",
   "metadata": {},
   "source": [
    "## Cost function : MSE\n",
    "### 기존 Simple Linear Regression 과 동일한 공식\n",
    "### cost(W) = mean(H(x^(i) - y^(i)))^2 // mean 평균\n",
    "x^(i) : Prediction, y^(i) : Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-binary",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-government",
   "metadata": {},
   "source": [
    "## Gradient Descent with \"torch.optim\"\n",
    "### ∇W = (d cost/d W) = 2mean(Wx^(i) - y^(i))x^(i)\n",
    "### W := W - α∇W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr = 1e - 5)\n",
    "\n",
    "# optimizer 사용법\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-dylan",
   "metadata": {},
   "source": [
    "cost를 구할 떄마다 optimizer의 gradient에 저장한 후 gradient descent를 시행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-webster",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-assignment",
   "metadata": {},
   "source": [
    "## Full Code with \"torch.optim\"\n",
    "- 점점 작아지는 Cost\n",
    "- 점점 y에 가까워지는 H(x)\n",
    "- Learning rate 에 따라 발산할수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tight-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
      "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
      "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
      "Epoch    4/20 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936005\n",
      "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371010\n",
      "Epoch    6/20 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812]) Cost: 29.758139\n",
      "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805]) Cost: 10.445305\n",
      "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391228\n",
      "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493135\n",
      "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
      "Epoch   11/20 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602]) Cost: 1.710541\n",
      "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651412\n",
      "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632387\n",
      "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625923\n",
      "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623412\n",
      "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622141\n",
      "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621253\n",
      "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963]) Cost: 1.620500\n",
      "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985]) Cost: 1.619770\n",
      "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000]) Cost: 1.619033\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 정의\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "\n",
    "# 2. 모델 초기화 (모델 정의)\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 3. optimizer 설정 (optimizer 정의)\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # 4. H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "\n",
    "    # 5. cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(),\n",
    "        cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-taiwan",
   "metadata": {},
   "source": [
    "데이터를 정의하는 부분과 W를 정의하는 부분만 달라짐\n",
    "\n",
    "학습한 부분이 Simple Linear Regression 과 동일 => PyTorch의 확장성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-margin",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-purpose",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-database",
   "metadata": {},
   "source": [
    "## nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# H(x) 계산\n",
    "hypothesis = x_train.matmul(W) + b # or .mm or @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-validation",
   "metadata": {},
   "source": [
    "- nn.Module 을 상속해서 모델 생성\n",
    "- nn.Linear(3, 1) =>    입력 차원 : 3,    출력 차원 : 1\n",
    "- Hypothesis 계산은 forward() 에서\n",
    "- Gradient 계산은 PyTorch가 알아서 해줌 backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-variance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "hypothesis = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-virgin",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-fraction",
   "metadata": {},
   "source": [
    "## F.mse_loss\n",
    "- torch.nn.functional 에서 제공하는 loss function 사용\n",
    "- 쉽게 다른 loss 와 교체 가능 (l1_loss, smooth_l1_loss 등....)\n",
    "\n",
    "PyTorch의 cost function을 사용하면 후에 다음 cost function을 바꿀 때 좀 더 편리하고, cost function을 계산하면서 생기는 버그가 없어 디버깅할 때 편리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost 계산\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.funtional as F\n",
    "\n",
    "#cost 계산\n",
    "cost = F.mse_loss(prediction, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-aggregate",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-fluid",
   "metadata": {},
   "source": [
    "## Full Code with \"torch.optim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 정의\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 2. 모델 초기화 (모델 정의)\n",
    "##W = torch.zeros((3, 1), requires_grad=True)\n",
    "##b = torch.zeros(1, requires_grad=True)\n",
    "model = MultivariateLinearRegressionModel()\n",
    "\n",
    "# 3. optimizer 설정 (optimizer 정의)\n",
    "optimizer = optim.SGD([W, b], lr = 1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # 4. H(x) 계산 (Hypothesis 계산)\n",
    "    hypothesis = x_train.matmul(W) + b # or .mm or @\n",
    "    Hypothesis = model(x_train)\n",
    "\n",
    "    # 5. cost 계산 (MSE)\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    # 6. cost로 H(x) 개선 (Gradient descent)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(),\n",
    "        cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-offer",
   "metadata": {},
   "source": [
    "모델을 정의하는 부분이 약간 바뀜\n",
    "\n",
    "__학습할 때 변화된 부분__\n",
    "- Hypothesis를 계산할 때 model 오브젝트 사용함\n",
    "- cost를 계산할 때 F.mse_loss라는 함수를 사용함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-influence",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-motivation",
   "metadata": {},
   "source": [
    "### 다음 시간\n",
    "\n",
    "많은 양의 데이터가 있지 않으면 정확한 예측이 어렵다.\n",
    "\n",
    " 특히 딥러닝의 경우 데이터의 양이 많을수록 좋은데, PyTorch에서는 메모리에 저장할 수 없을 정도의 양의 데이터를 어떻게 다루는가?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
