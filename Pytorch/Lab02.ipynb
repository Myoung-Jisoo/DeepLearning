{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "english-doubt",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-italy",
   "metadata": {},
   "source": [
    "학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinguished-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-player",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-pressing",
   "metadata": {},
   "source": [
    "## Data definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-proof",
   "metadata": {},
   "source": [
    "Training dataset : 학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별하기 위한 데이터\n",
    "\n",
    "Test dataset     : 예상값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-fountain",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-hello",
   "metadata": {},
   "source": [
    "모델을 학습시키기 위한 데이터는 torch.tensor 에 있음\n",
    "\n",
    "입력은 __x.train__, 출력은 __y.train__ 으로 각기 다른 tensor에 저장하고 입출력을 __x, y__ 로 구분한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recorded-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-bidding",
   "metadata": {},
   "source": [
    "## \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-morrison",
   "metadata": {},
   "source": [
    "## Hypothesis (모델)\n",
    "### y = Wx + b\n",
    "#### W : Weight,  b : Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "above-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "# W와 b를 0으로 초기화 (항상 출력 0으로 예측)\n",
    "# requires_grad=True  : W와 b를 학습시킬 것이라 명시\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-thesaurus",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-daily",
   "metadata": {},
   "source": [
    "## Comput lose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-agent",
   "metadata": {},
   "source": [
    "loss / cost : 우리의 모델이 얼마나 정답에 가까운가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-arbor",
   "metadata": {},
   "source": [
    "### MSE(Mean Squared Error) : loss 계산 함수\n",
    "### cost(W, b) = mean(H(x^i) - y^i)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "political-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2) # torch.mean을 이용해 평균 계산 (한 줄이라 읽기 편함)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-michigan",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "under-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "hypothesis = x_train * W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-deficit",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-genesis",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "### 계산한 loss 를 이용해 model 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-sentence",
   "metadata": {},
   "source": [
    "__torch.optim 라이브러리 사용__\n",
    "\n",
    "- [W, b]는 학습할 tensor들\n",
    "\n",
    "- lr = 0.01 은 learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-adams",
   "metadata": {},
   "source": [
    "__항상 붙어다니는 3줄__\n",
    "- zero_grad() 로 gradient 초기화\n",
    "- backward() 로 gradient 계산\n",
    "- step() 으로 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "operational-shannon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr = 0.01) # SFD (Stochastic Gradient Descent)\n",
    "# 학습시킬 변수 W와 b 이 두 개를 list로 넣어줌\n",
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step() # 계산된 gradient 방향대로 Weight와 Bias, W와 b 계산\n",
    "# optimizer를 이용해 학습시킬 때 필요한 3줄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-registration",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-thailand",
   "metadata": {},
   "source": [
    "## 전체 코드 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numerous-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "# data인 x_train과 y_train 정의\n",
    "\n",
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "# Hypothesis에 필요한 W(Weight) 와 b(Bias) 초기화\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 0.01)\n",
    "# PyTorch를 이용해 Optimizer 정의\n",
    "\n",
    "############### 학습할 준비 완료 ###############\n",
    "\n",
    "nb_epochs = 1000 ## 학습을 원하는 만큼 for문 돌리기\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    hypothesis = x_train * W + b\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    # 한 번 돌 때마다 model로 예측하고, 예측값과 y값의 차이를 이용해 cost를 계산한 후\n",
    "    # 계산한 gradient를 이용해 optimezer로 학습\n",
    "    \n",
    "    ## 반복해서 학습을 하다보면 W와 b가 각각 하나의 숫자로 수렴한다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
