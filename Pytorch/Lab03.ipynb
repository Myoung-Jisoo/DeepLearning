{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "english-doubt",
   "metadata": {},
   "source": [
    "# Deeper Look at Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinguished-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-bidding",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-morrison",
   "metadata": {},
   "source": [
    "## Hypothesis (Linear Regression) : 인공 신경망의 구조를 나타냄\n",
    "### H(x) = Wx + b  => W와 b라는 변수를 학습해 주어진 데이터에 최적화\n",
    "#### W : Weight,  b : Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-antenna",
   "metadata": {},
   "source": [
    "주어진 input x에 대해 어떤 output y를 예측할지 알려주며 H(x)로 표현\n",
    "\n",
    "W : 하나의 Matrix\n",
    "\n",
    "b : vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "above-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True) # W와 b 초기화\n",
    "hypothesis = x_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-michigan",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-barrel",
   "metadata": {},
   "source": [
    "## Simpler Hypothesis Function (Bias를 삭제한 더 간단한 모델)\n",
    "## W라는 상수 하나만 학습 가능한 모델이다\n",
    "### H(x) = Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "under-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "# b = torch.zeros(1, requires_grad = True)\n",
    "hypothesis = x_train * W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-deficit",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-genesis",
   "metadata": {},
   "source": [
    "## Dummy Data\n",
    "### Input = Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "operational-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) # 입력과 출력의 값이 동일\n",
    "# PyTorch에서는 dataset을 위와 같이 torch.Float() 2개로 표현할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-aircraft",
   "metadata": {},
   "source": [
    "입력과 출력이 동일하므로 최적의 Hypothesis function은 H(x) = x 이다. => W = 1일 때 모든 데이터의 정확한 값을 예측할 수 있음\n",
    "\n",
    "==> W != 1 일 경우, 학습의 목표는 W를 1로 수렴시키는 것이다.\n",
    "\n",
    "==> W가 1에 가까울수록 정확한 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-martin",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-windows",
   "metadata": {},
   "source": [
    "### 모델의 좋고 나쁨을 어떻게 평가하는가? => Cost funtion : Intuition 정의\n",
    "cost funtion : 모델 예측값이 실제 데이터와 얼마나 다른지 나타내는 값 => 잘 학습된 모델일수록 낮은 cost를 가짐\n",
    "\n",
    "ex) 바로 위의 모델에서는 W = 1일 때 cost = 0 => cost는 아래로 볼록한 포물선 모양을 그래프를 가지게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-short",
   "metadata": {},
   "source": [
    "### Linear Regression에서 쓰이는 cost funtion은 MSE(Mean Squared Error)라고 함\n",
    "### MSE : 예측값과 실제값의 차이를 제곱한 평균을 구해줌\n",
    "__Pytorch로 MSE 구하는 방법__\n",
    "- 차이 구하기 : hypothesis - y_train\n",
    "- (차이)^2 : 차이 제곱\n",
    "- torch.mean 함수 이용해 dataset 전체의 평균 구하기\n",
    "\n",
    "\n",
    "Cost funtion : 예측값과 실제값의 차이를 나타냄 / gradient : 기울기\n",
    "__Cost funtion 을 최소화__ 하는 것이 목표\n",
    "- 기울기 음수 -> W가 더 커져야 함\n",
    "- 기울기 양수 -> W가 더 작아져야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-waters",
   "metadata": {},
   "source": [
    "- 기울기가 가파를수록 cost가 큼 -> W를 크게 바꾸기\n",
    "- 평평할수록 cost가 0에 가까움  -> W를 조금 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-hypothetical",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-improvement",
   "metadata": {},
   "source": [
    "## Gradient Descent : The Math  기울기 계산하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-constitutional",
   "metadata": {},
   "source": [
    "### 미분 필요! => cost funtion은 W에 대한 2차 함수 -> 간단한 미분 방정식 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-balloon",
   "metadata": {},
   "source": [
    "Gradient descent : Gradient를 이용해 Cost를 줄인다\n",
    "\n",
    "## W := W - α∇W\n",
    "### W : Gradient, α : Learning rate (상수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-baker",
   "metadata": {},
   "source": [
    "__Gradient Descent를 PyTorch로 구현하기__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "numeric-ability",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-13bb5b4975b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dataset의 전체 평균 gradient 구하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m \u001b[1;31m# 상수 정의 // 코드에서 통상적으로 learning rate를 lr이라고 줄여 쓴다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;31m# 정의한 상수대로 W 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "gradient = 2 * torch.mean((W * x_train - y_train) * x_train) # dataset의 전체 평균 gradient 구하기\n",
    "lr = 0.1 # 상수 정의 // 코드에서 통상적으로 learning rate를 lr이라고 줄여 쓴다.\n",
    "W -= lr * gradient # 정의한 상수대로 W 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-presentation",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-testament",
   "metadata": {},
   "source": [
    "## 신경망에 사용할 Full Code 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-invasion",
   "metadata": {},
   "source": [
    "학습할수록 W가 1에 수렴 (학습, 반복할 때마다 dataset 전체를 사용)\n",
    "\n",
    "cost 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1)\n",
    "\n",
    "# learning rate 설정\n",
    "lr = 0.1\n",
    "\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W\n",
    "\n",
    "    # cost gradient 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
    "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}'.format( # Epoch : 데이터로 학습한 횟수\n",
    "         epoch, nb_epochs, W.item(), cost.item()\n",
    "     ))\n",
    " \n",
    "    # cost gradient로 H(x) 개선\n",
    "     W -= lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-darkness",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-opportunity",
   "metadata": {},
   "source": [
    "## Gradient Descent with \"torch.optim\"\n",
    "### __torch.optim__ 으로 __gradient descent__ 를 할 수 있음\n",
    "optimizer를 정의하기 위해서는 학습가능한 변수들과 learning weight를 알아야 함\n",
    "\n",
    "모델 :  H(x) = Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer 설정 // cost funtion 계산\n",
    "optimizer = optim.SGD([W], lr = 0.15)\n",
    "\n",
    "# cost로 H(x) 개선 // Gradient descent 수행\n",
    "optimezer.zero_grad() # optimizer에 있는 모든 학습 가능한 변수의 gradient를 전부 0으로 초기화\n",
    "cost.backward()       # cost function을 미분해 각 변수들의 gradient 채우기\n",
    "optimizer.step()      # 저장된 gradient 값으로 gradient descent 시행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-outside",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-desperate",
   "metadata": {},
   "source": [
    "## Full Code with \"torch.optim\"\n",
    "### gradient decent를 torch.optim 을 사용한 코드로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W], lr=0.15)\n",
    "\n",
    "\n",
    "nb_epochs = 10\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    print('Epoch {:4d}/{} W: {:.3f} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, W.item(), cost.item()\n",
    "    ))\n",
    "    \n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
